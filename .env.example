# Copy this file to ".env" at the repo root and adjust values as needed.
# All deploy scripts read from the top-level .env.
#
# IMPORTANT: Do not commit real secrets. Keep tokens/passwords out of version control.

############################################
# Common
############################################
# Default OpenShift namespace used by most components
NAMESPACE=llama-stack-demo
# Optional: Adjust logging verbosity for services (if they read LOG_LEVEL)
LOG_LEVEL=INFO

############################################
# Special Project App (special_project_app)
############################################
SERVICENOW_INSTANCE_URL=https://example.service-now.com
SERVICENOW_USERNAME=changeme
SERVICENOW_PASSWORD=changeme

############################################
# Kubernetes MCP Server (k8s_mcp_server)
############################################
DEPLOY_NAMESPACE=llama-stack-demo
TARGET_NAMESPACE=special-payment-project
K8S_APP_NAME=kubernetes-mcp-server
IMAGE=quay.io/manusa/kubernetes_mcp_server
IMAGE_TAG=latest
LOG_LEVEL=2
READ_ONLY=true
CPU_REQUEST=50m
MEMORY_REQUEST=64Mi
CPU_LIMIT=200m
MEMORY_LIMIT=256Mi

############################################
# Llama Stack Distribution (llama-stack)
############################################
# Name of the LlamaStackDistribution resource
LSD_NAME=lsd-llama-milvus-inline
# Inference model identifier consumed by the server (stored in a Secret)
INFERENCE_MODEL=llama-4-scout-17b-16e-w4a16

# Remote vLLM endpoint config (stored in a Secret; required if using remote vLLM)
VLLM_URL=https://vllm.example.com/v1
VLLM_API_TOKEN=changeme
VLLM_TLS_VERIFY=false

# Vector Store for RAG
# Do not update manually - the llama stack script will populate this.
# This variable is used by the confluence ingestor to ingest the docs into the below target vector store
# It is also used by the k8s diagnostics agent as the store it should look to for RAG.
VECTOR_DB_ID=vs_your-vector-store-id

############################################
# Confluence Ingestor (confluence_ingestor)
############################################
CONFLUENCE_INGESTOR_APP_NAME=aiops-conf-ingestion
LLAMA_BASE_URL=http://lsd-llama-milvus-inline-service.llama-stack-demo.svc.cluster.local:8321
CONF_CLOUD_ID=your-confluence-cloud-id
CONF_USER=your-confluence-user
CONF_API_TOKEN=changeme
SPACE_NAME="Special Payment Project"

############################################
# K8s Diagnostics Agent (k8s_diagnostics_agent)
############################################
LLAMA_BASE_URL=http://lsd-llama-milvus-inline-service.llama-stack-demo.svc.cluster.local:8321
# Route timeout annotation value (used only by the deploy script)
ROUTE_TIMEOUT=5m

############################################
# AAP (ansible_deployment)
############################################
AAP_HOSTNAME=https://aap.example.com
AAP_TOKEN=changeme
AAP_VALIDATE_CERTS=false
AAP_ORG=Default

# ServiceNow Secret
SN_INSTANCE=https://example.service-now.com
SN_USERNAME=changeme
SN_PASSWORD=changeme

# Openshift Secret
OCP_API_HOST=https://api.example.openshift.com:6443
# If the openshift cluster has been restarted. The token will be replaced so run this comand to get the new token.
# oc -n special-payment-project get secret aap-automation-token -o jsonpath='{.data.token}' | base64 -d
# Or you could rerun ./special_project_app/scripts/oc-deploy.sh and it will replace the token in this file automatically.
OCP_API_TOKEN=replace-with-openshift-token
